{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#\n","# The task is to develop an algorithm to disambiguate people contributing to various scientific events (e.g. talks, presentations, sessions).\n","#\n","# For this, you have to create one profile per person. This profile will have all the contribution from that person (100% recall) and no other contributions from anyone else (100% precision).\n","#\n","# Please avoid unnecessary duplicates as well as mixing contributions from different scientists despite similar names/focus-areas.\n","\n","# Note this is less a word sense disambigation task\n","# just entity linking\n","\n","# https://link.springer.com/article/10.1007/s11192-021-03951-w\n","# Some sort of larger approach with knowledge base would help\n","# espciallu some knowledge base approaches\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import random\n","import string\n","\n","import matplotlib.pyplot as plt\n","import numpy as np  # linear algebra\n","import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n","import seaborn as sns\n","# from spacy.pipeline.entity_linker import DEFAULT_NEL_MODEL\n","import spacy\n","import xgboost as xgb\n","from sklearn import ensemble, metrics, model_selection, naive_bayes\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics import accuracy_score, f1_score,mean_squared_error\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","from sklearn.model_selection import RandomizedSearchCV\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["%pylab is deprecated, use %matplotlib inline and import the required libraries.\n","Populating the interactive namespace from numpy and matplotlib\n"]},{"name":"stderr","output_type":"stream","text":["/home/ubuntu/anaconda3/envs/productclassifer/lib/python3.8/site-packages/IPython/core/magics/pylab.py:162: UserWarning: pylab import has clobbered these variables: ['random']\n","`%matplotlib` prevents importing * from pylab and numpy\n","  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"]}],"source":["%matplotlib inline\n","%pylab inline\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# !pip install spacy==2.2.4\n","# !python -m spacy download en_core_web_lg"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["df = pd.read_json(\"data.json\")\n","# 1. data.json: List of 5086 various contributions, described by several attributes (features), e.g. names, information about the workplace of the author, its geolocation,\n","# and focus areas (key topics covered in contribution)\n","df_ground_truth = pd.read_json(\"ground-truth.json\")\n","# 2. ground_truth.json: \"Ground truth\" - actual group s of contributions from the data file\n","# #(each contribution is assigned to a person)\n","df_person = pd.read_json(\"persons.json\")  # do i need this to map back?\n","# 3. persons.json: The list of unique people.\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# label df with ground_truth for easier work\n","df =df.join(df_ground_truth.set_index('contributionId'), on='contribution_id')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["df['str_focus_areas'] = [','.join(map(str, l)) for l in df['focus_areas']]\n","df['str_gpes'] = [','.join(map(str, l)) for l in df['focus_areas']]\n","df['str_orgs'] = [' '.join(map(str, l)) for l in df['focus_areas']]\n","\n","df[\"features\"] = df[[\"first_name\",\"middle_name\",\"last_name\",\"workplace\",\"str_focus_areas\",\"str_gpes\",\"str_orgs\"]].agg(' '.join, axis=1)\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# contribution distribution\n","#df_ground_truth.groupby(\"personId\").count().sort_values(by=['contributionId']).plot()\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# With this data I don't have free text so remember entity linkers are going to fail more\n","# This maybe could benefit from a knowledge graph problem.\n","# Treating like wiki disambiguation "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["#  focus only on one prior topics for now\n","# this does drop 1000 items\n","#df[\"focus_areas\"] = df[df[\"focus_areas\"].map(lambda d: len(d)) > 0]\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Let's just use names see what is missed\n","#nlp = spacy.load(\"en_core_web_lg\")\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["df['cm_full_name'] = df[[\"first_name\",\"middle_name\",\"last_name\"]].agg(' '.join, axis=1)\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","author_mapping_dict = { label:idx for idx,label in enumerate(df[\"personId\"])}\n","id_author_mapping_dict = dict((v, k) for k, v in author_mapping_dict.items())\n","\n","d_labels = df[\"personId\"].map(author_mapping_dict)\n","\n","X_train, X_test, y_train, y_test = train_test_split(df[\"features\"], d_labels, test_size=0.33, random_state=42)\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Unique persons\n","n_persons = len(df_person)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["## Fit transform the tfidf vectorizer \n","tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n","full_tfidf = tfidf_vec.fit_transform(X_train.values.tolist() + X_test.values.tolist())\n","train_tfidf = tfidf_vec.transform(X_train.values.tolist())\n","test_tfidf = tfidf_vec.transform(X_test.values.tolist())\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# raw tokens\n","# We do lose any order with those feature columns with below method however has no effect anyway.\n","vectorizer = CountVectorizer()\n","full_vec = vectorizer.fit_transform(X_train.values.tolist() + X_test.values.tolist())\n","train_vec = vectorizer.transform(X_train.values.tolist())\n","test_vec = vectorizer.transform(X_test.values.tolist())\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# This won't give better performance as the features have no textual semantics in them and are just text labels.\n","# model = SentenceTransformer('distilbert-base-uncased')\n","# train_embeddings = model.encode(X_train.values.tolist())\n","# test_embeddings = model.encode(X_test.values.tolist())\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["\n","               \n","# params to avoid overfitting\n","# model = RandomForestClassifier()\n","# model.fit(train_tfidf, y_train)\n","\n","# #%%\n","# y_pred = model.predict(test_tfidf)\n","# print(f1_score(y_test, y_pred, average='macro'))\n","# print(accuracy_score(y_test, y_pred))"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["model = RandomForestClassifier(n_estimators=50, min_samples_leaf=25)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["RandomForestClassifier(min_samples_leaf=25, n_estimators=50)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(train_vec, y_train)\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RF\n","0.11687880468069527\n","0.48392857142857143\n"]}],"source":["y_pred = model.predict(test_vec)\n","print(\"RF\")\n","\n","print(f1_score(y_test, y_pred, average='macro'))\n","print(accuracy_score(y_test, y_pred))\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Eval fit - how much overfitting? not really standardise labels. O\n","# mse_train = mean_squared_error(y_train, model.predict(train_vec))\n","# mse_test = mean_squared_error(y_test, y_pred)\n","# print(\"RF with full trees, Train MSE: {} Test MSE: {}\".format(mse_train, mse_test))\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["model = RandomForestClassifier()\n","\n","# Random search of parameters, using 3 fold cross validation, \n","# search across 100 different combinations, and use all available cores\n","# Found with testing adnom forest est approach and most likely because of the closely similar trees\n","n_estimators = [16,32,64,128]\n","max_features = ['auto', 'sqrt']\n","max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n","max_depth.append(None)\n","min_samples_split = [2, 5, 10]\n","min_samples_leaf = [1, 2, 4]\n","bootstrap = [True, False]\n","random_grid = {'n_estimators': n_estimators,\n","               'max_features': max_features,\n","               'max_depth': max_depth,\n","               'min_samples_split': min_samples_split,\n","               'min_samples_leaf': min_samples_leaf,\n","               'bootstrap': bootstrap}\n","\n","rf_random = RandomizedSearchCV(estimator = model,param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"]},{"name":"stderr","output_type":"stream","text":["/home/ubuntu/anaconda3/envs/productclassifer/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END bootstrap=False, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=32; total time=   0.5s\n","[CV] END bootstrap=False, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=32; total time=   0.5s\n","[CV] END bootstrap=True, max_depth=90, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=128; total time=   1.5s\n","[CV] END bootstrap=False, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=32; total time=   0.6s\n","[CV] END bootstrap=True, max_depth=90, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=128; total time=   1.6s\n","[CV] END bootstrap=True, max_depth=90, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=128; total time=   1.6s\n","[CV] END bootstrap=True, max_depth=90, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=16; total time=   0.2s\n","[CV] END bootstrap=True, max_depth=90, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=16; total time=   0.2s\n","[CV] END bootstrap=True, max_depth=90, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=16; total time=   0.2s\n","[CV] END bootstrap=False, max_depth=90, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=64; total time=   1.1s\n","[CV] END bootstrap=False, max_depth=90, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=64; total time=   1.2s\n","[CV] END bootstrap=False, max_depth=90, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=64; total time=   1.1s\n","[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=128; total time=   0.7s\n","[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=128; total time=   0.8s\n","[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=128; total time=   0.7s\n","[CV] END bootstrap=True, max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=128; total time=   0.7s\n","[CV] END bootstrap=False, max_depth=60, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=128; total time=   4.1s\n","[CV] END bootstrap=False, max_depth=60, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=128; total time=   3.6s\n","[CV] END bootstrap=False, max_depth=60, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=128; total time=   3.5s\n","[CV] END bootstrap=True, max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=128; total time=   0.7s\n","[CV] END bootstrap=True, max_depth=10, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=128; total time=   0.7s\n","[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=16; total time=   0.3s\n","[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=16; total time=   0.2s\n","[CV] END bootstrap=True, max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=16; total time=   0.2s\n","[CV] END bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=16; total time=   0.1s\n","[CV] END bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=16; total time=   0.1s\n","[CV] END bootstrap=False, max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=16; total time=   0.1s\n","[CV] END bootstrap=True, max_depth=110, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=16; total time=   0.3s\n","[CV] END bootstrap=True, max_depth=110, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=16; total time=   0.3s\n","[CV] END bootstrap=True, max_depth=110, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=16; total time=   0.3s\n","[CV] END bootstrap=False, max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=16; total time=   0.2s\n","[CV] END bootstrap=False, max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=16; total time=   0.2s\n","[CV] END bootstrap=False, max_depth=90, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=32; total time=   0.8s\n","[CV] END bootstrap=False, max_depth=90, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=32; total time=   0.8s\n","[CV] END bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=16; total time=   0.2s\n","[CV] END bootstrap=False, max_depth=90, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=32; total time=   0.9s\n","[CV] END bootstrap=False, max_depth=50, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=16; total time=   0.3s\n","[CV] END bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=16; total time=   0.2s\n","[CV] END bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=16; total time=   0.2s\n","[CV] END bootstrap=True, max_depth=60, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=32; total time=   0.4s\n","[CV] END bootstrap=True, max_depth=60, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=32; total time=   0.4s\n","[CV] END bootstrap=True, max_depth=60, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=32; total time=   0.4s\n","[CV] END bootstrap=True, max_depth=70, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=64; total time=   0.6s\n","[CV] END bootstrap=True, max_depth=70, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=64; total time=   0.6s\n","[CV] END bootstrap=True, max_depth=70, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=64; total time=   0.7s\n","[CV] END bootstrap=True, max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=64; total time=   0.8s\n","[CV] END bootstrap=True, max_depth=80, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=32; total time=   0.3s\n","[CV] END bootstrap=True, max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=64; total time=   0.8s\n","[CV] END bootstrap=True, max_depth=80, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=32; total time=   0.3s\n","[CV] END bootstrap=True, max_depth=80, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=32; total time=   0.3s\n","[CV] END bootstrap=True, max_depth=50, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=64; total time=   0.9s\n","[CV] END bootstrap=False, max_depth=110, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=32; total time=   0.5s\n","[CV] END bootstrap=False, max_depth=110, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=32; total time=   0.5s\n","[CV] END bootstrap=False, max_depth=110, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=32; total time=   0.5s\n","[CV] END bootstrap=False, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=64; total time=   1.6s\n","[CV] END bootstrap=False, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=64; total time=   1.5s\n","[CV] END bootstrap=False, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=64; total time=   1.6s\n","[CV] END bootstrap=True, max_depth=80, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=16; total time=   0.3s\n","[CV] END bootstrap=False, max_depth=90, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=128; total time=   1.8s\n","[CV] END bootstrap=True, max_depth=80, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=16; total time=   0.2s\n","[CV] END bootstrap=True, max_depth=80, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=16; total time=   0.2s\n"]}],"source":["rf_random.fit(train_vec, y_train)\n","model = rf_random"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_pred = model.predict(test_vec)\n","print(\"Random search + RF\")\n","print(f1_score(y_test, y_pred, average='macro'))\n","print(accuracy_score(y_test, y_pred))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# do on original data for output reasns + sharing\n","d_vec = vectorizer.transform(df[\"features\"].values.tolist())\n","\n","np.argmax(model.predict_proba(d_vec),axis=1)\n","\n","# res_prob = model.predict_proba(d_vec)\n","\n","\n","y_res = [id_author_mapping_dict[i] for i in d_labels]\n","# different metrics but might be able to get a conf out of a randomforrest\n","# y_conf = [res_prob[idx][m_i] for idx, m_i in enumerate([np.argmax(model.predict_proba(d_vec),axis=1)])][0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"pred\"] = y_res\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.to_csv(\"full_output_matches.tsv\",sep ='\\t', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"index\",\"contribution_id\",\"features\",\"personId\",\"pred\"]].to_csv(\"output_matches.tsv\",sep ='\\t', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[[\"index\",\"contribution_id\",\"features\",\"personId\",\"pred\"]][:25].to_csv(\"sample_matches.tsv\",sep ='\\t', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"80a466fdeb932694bd657d66ac7cbf062f33a778cb791e0ddc56044673225083"},"kernelspec":{"display_name":"Python 3.8.12 64-bit ('productclassifer': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
